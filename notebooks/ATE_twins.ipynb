{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "for path in [\"../causalml\", \"../\"]:\n",
    "    module_path = os.path.abspath(os.path.join(path))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from causalml.inference.meta import BaseDRLearner, BaseTLearner, BaseXRegressor\n",
    "from causalml.propensity import GradientBoostedPropensityModel\n",
    "from src.data.data_module import generate_data_exp\n",
    "from src.data.utils import split_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor(missing_value):\n",
    "    return XGBRegressor(missing=missing_value, eval_metric=\"logloss\")\n",
    "\n",
    "\n",
    "learners = {\n",
    "    \"T\": lambda missing_value: BaseTLearner(learner=get_regressor(missing_value)),\n",
    "    \"X\": lambda missing_value: BaseXRegressor(learner=get_regressor(missing_value)),\n",
    "    \"DR\": lambda missing_value: BaseDRLearner(\n",
    "        learner=get_regressor(missing_value),\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def xgb_prop(missing_value, X, W):\n",
    "    pm = GradientBoostedPropensityModel(eval_metric=\"logloss\", missing=missing_value)\n",
    "    return pm.fit_predict(X, W)\n",
    "\n",
    "\n",
    "prop_learners = {\n",
    "    \"none\": lambda missing_value, X, W: None,\n",
    "    \"xgb\": lambda missing_value, X, W: xgb_prop(missing_value, X, W),\n",
    "}\n",
    "\n",
    "\n",
    "def get_imputer(missing_value):\n",
    "    return IterativeImputer(\n",
    "        max_iter=1500, tol=15e-4, random_state=None, missing_values=missing_value\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833822fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP. SETTINGS\n",
    "runs = 100\n",
    "sims = 10\n",
    "\n",
    "assert runs % sims == 0\n",
    "\n",
    "n = 5000\n",
    "d = 20\n",
    "z_d_dim = 10\n",
    "amount_of_missingness = 0.3\n",
    "missing_value = -1\n",
    "\n",
    "learner = \"X\"\n",
    "prop_learner = \"xgb\"\n",
    "\n",
    "data = \"twins\"\n",
    "\n",
    "# DEBUG SETTINGS\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449266ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "\n",
    "ATE_impute_all = []\n",
    "ATE_impute_nothing = []\n",
    "ATE_impute_smartly = []\n",
    "ATE_impute_wrongly = []\n",
    "\n",
    "for _ in tqdm(range(sims)):\n",
    "    X, X_, Y0, Y1, Y, CATE, W, Z_up, Z_down = generate_data_exp(\n",
    "        n * 2, d, z_d_dim, amount_of_missingness, missing_value=missing_value, data=data\n",
    "    )\n",
    "\n",
    "    for _ in tqdm(range(int(runs / sims)), leave=False):\n",
    "        idxs = np.random.choice(range(n * 2), size=n, replace=False)\n",
    "\n",
    "        X__i, Y_i, W_i = X_[idxs], Y[idxs], W[idxs]  # 50% fold -> n=n\n",
    "\n",
    "        ground_truth.append(Y1.mean() - Y0.mean())\n",
    "\n",
    "        # IMPUTE ALL\n",
    "        X_in_use = X__i.copy()\n",
    "        imputer = get_imputer(missing_value)\n",
    "        imputer.fit(X_in_use)\n",
    "        X_in_use = imputer.transform(X_in_use)\n",
    "\n",
    "        cm_impute_all = learners[learner](missing_value)\n",
    "        ps = prop_learners[prop_learner](missing_value, X_in_use, W_i)\n",
    "\n",
    "        ATE_impute_all.append(cm_impute_all.estimate_ate(X_in_use, W_i, Y_i, p=ps)[0])\n",
    "\n",
    "        if verbose:\n",
    "            print(\"all\", X_in_use.min())\n",
    "\n",
    "        # IMPUTE NOTHING\n",
    "        X_in_use = X__i.copy()\n",
    "\n",
    "        cm_impute_nothing = learners[learner](missing_value)\n",
    "        ps = prop_learners[prop_learner](missing_value, X_in_use, W_i)\n",
    "\n",
    "        ATE_impute_nothing.append(\n",
    "            cm_impute_nothing.estimate_ate(X_in_use, W_i, Y_i, p=ps)[0]\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"nothing\", X_in_use.min())\n",
    "\n",
    "        # IMPUTE SMARTLY\n",
    "        X_in_use = X__i.copy()\n",
    "        imputer_smart = get_imputer(missing_value)\n",
    "        imputer_smart.fit(X_in_use[:, z_d_dim:])\n",
    "\n",
    "        X_in_use[:, z_d_dim:] = imputer_smart.transform(X_in_use[:, z_d_dim:])\n",
    "\n",
    "        est_impute_smartly = learners[learner](missing_value)\n",
    "        ps = prop_learners[prop_learner](missing_value, X_in_use, W_i)\n",
    "\n",
    "        ATE_impute_smartly.append(\n",
    "            est_impute_smartly.estimate_ate(X_in_use, W_i, Y_i, p=ps)[0]\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"smart down\", X_in_use[:, :z_d_dim].min())\n",
    "            print(\"smart up\", X_in_use[:, z_d_dim:].min())\n",
    "\n",
    "        # IMPUTE WRONGLY\n",
    "        X_in_use = X__i.copy()\n",
    "        imputer_wrongly = get_imputer(missing_value)\n",
    "        imputer_wrongly.fit(X_in_use[:, :z_d_dim])\n",
    "\n",
    "        X_in_use[:, :z_d_dim] = imputer_wrongly.transform(X_in_use[:, :z_d_dim])\n",
    "\n",
    "        est_impute_wrongly = learners[learner](missing_value)\n",
    "        ps = prop_learners[prop_learner](missing_value, X_in_use, W_i)\n",
    "\n",
    "        ATE_impute_wrongly.append(\n",
    "            est_impute_wrongly.estimate_ate(X_in_use, W_i, Y_i, p=ps)[0]\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"wrong down\", X_in_use[:, :z_d_dim].min())\n",
    "            print(\"wrong up\", X_in_use[:, z_d_dim:].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# SETUP\")\n",
    "print(f\"# DATA = {data}\")\n",
    "print(f\"# learner = {learner}\")\n",
    "print(f\"# amount_of_missingness = {amount_of_missingness}\")\n",
    "print(f\"# z_d_dim = {z_d_dim}\")\n",
    "print(f\"# amount of runs = {runs}\")\n",
    "\n",
    "all_means, all_stds = split_eval(ATE_impute_all, ground_truth, sims)\n",
    "no_means, no_stds = split_eval(ATE_impute_nothing, ground_truth, sims)\n",
    "smart_means, smart_stds = split_eval(ATE_impute_smartly, ground_truth, sims)\n",
    "wrong_means, wrong_stds = split_eval(ATE_impute_wrongly, ground_truth, sims)\n",
    "\n",
    "print(f\"#   ALL IMPUTATION   :\\t {all_means.mean()}\\t{all_stds.mean()}\")\n",
    "print(f\"#   NO IMPUTATION    :\\t {no_means.mean()}\\t{no_stds.mean()}\")\n",
    "print(f\"#   SMART IMPUTATION :\\t {smart_means.mean()}\\t{smart_stds.mean()}\")\n",
    "print(f\"#   WRONG IMPUTATION :\\t {wrong_means.mean()}\\t{wrong_stds.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# DATA = twins\n",
    "# learner = T\n",
    "# amount_of_missingness = 0.3\n",
    "# z_d_dim = 10\n",
    "# amount of runs = 100\n",
    "#   ALL IMPUTATION   :\t 5.7554849319306065\t2.5355438478863563\n",
    "#   NO IMPUTATION    :\t 6.338256012605365\t3.156546417385256\n",
    "#   SMART IMPUTATION :\t 3.8482661724632488\t2.0178561822657173\n",
    "#   WRONG IMPUTATION :\t 6.530310658947888\t3.150978112501435\n",
    "\n",
    "# SETUP\n",
    "# DATA = twins\n",
    "# learner = DR\n",
    "# amount_of_missingness = 0.3\n",
    "# z_d_dim = 10\n",
    "# amount of runs = 100\n",
    "#   ALL IMPUTATION   :\t 8.975225169035607\t3.0473058082213367\n",
    "#   NO IMPUTATION    :\t 10.31051638371869\t4.182175835788135\n",
    "#   SMART IMPUTATION :\t 5.754394464820508\t2.4033580089100903\n",
    "#   WRONG IMPUTATION :\t 9.297950298812191\t3.739133997820358\n",
    "\n",
    "\n",
    "# SETUP\n",
    "# DATA = twins\n",
    "# learner = X\n",
    "# amount_of_missingness = 0.3\n",
    "# z_d_dim = 10\n",
    "# amount of runs = 100\n",
    "#   ALL IMPUTATION   :\t 30.792042486211823\t8.45980458171202\n",
    "#   NO IMPUTATION    :\t 8.980250054602692\t5.661282026773433\n",
    "#   SMART IMPUTATION :\t 8.153414737744438\t4.562703649784634\n",
    "#   WRONG IMPUTATION :\t 10.369293354405885\t6.097239453615901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ea32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
