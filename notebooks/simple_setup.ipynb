{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from econml import metalearners\n",
    "from econml.dml import NonParamDML\n",
    "from econml.metalearners import XLearner\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "    n, d, z_d_dim, amount_of_missingness, treatment_balance, missing_value=-1\n",
    "):\n",
    "    # GENERATE DATA\n",
    "\n",
    "    assert 0 < n\n",
    "    assert 0 < z_d_dim <= d\n",
    "    assert 0 < amount_of_missingness < 0.5\n",
    "    assert 0.5 <= treatment_balance < 1.0\n",
    "\n",
    "    # COVARIATES\n",
    "    # X = np.random.rand(n, drandom.multivariate_normal)         # Fully observed X\n",
    "    A = np.random.rand(d, d)\n",
    "    cov = np.dot(A, A.transpose())\n",
    "\n",
    "    X = np.random.multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    X /= X.max() - X.min()\n",
    "\n",
    "    # DOWN\n",
    "    alpha = 1 - amount_of_missingness\n",
    "    p = (1 + np.sqrt(2 * alpha - 1)) / 2\n",
    "\n",
    "    theta_down = np.random.rand(z_d_dim)\n",
    "    Z_down = np.logical_xor(\n",
    "        X[:, :z_d_dim] + np.random.randn(n, z_d_dim) * 0.01 > 0, theta_down > p\n",
    "    ).astype(int)\n",
    "    Z_down = np.abs(Z_down - 1)  # 0 = missing, 1 = present\n",
    "\n",
    "    # X_tilde_down\n",
    "    X_ = X.copy()\n",
    "    X_[:, :z_d_dim][Z_down == 0] = missing_value\n",
    "\n",
    "    # X^down, Z^down -> W\n",
    "    # TREATMENTS\n",
    "    theta_w = np.random.rand(z_d_dim)\n",
    "    p = (1 + np.sqrt(2 * 0.6 - 1)) / 2\n",
    "    _B = np.logical_xor(\n",
    "        X[:, :z_d_dim] + np.random.randn(n, z_d_dim) * 0.01 > 0, theta_w > p\n",
    "    ).astype(int)\n",
    "    _B = np.abs(_B - 1).mean(1)\n",
    "    W = np.random.binomial(1, _B)\n",
    "\n",
    "    # UP\n",
    "    theta_up_1 = np.random.rand(d - z_d_dim)\n",
    "    theta_up_0 = np.random.rand(d - z_d_dim)\n",
    "\n",
    "    alpha = 1 - amount_of_missingness\n",
    "    p = (1 + np.sqrt(2 * alpha - 1)) / 2\n",
    "\n",
    "    Z_up_0 = np.logical_xor(\n",
    "        X[np.where(W == 0)[0], z_d_dim:] > 0, theta_up_0 > p\n",
    "    ).astype(int)\n",
    "    Z_up_0 = np.abs(Z_up_0 - 1)\n",
    "\n",
    "    Z_up_1 = np.logical_xor(\n",
    "        X[np.where(W == 1)[0], z_d_dim:] > 0, theta_up_1 > p\n",
    "    ).astype(int)\n",
    "    Z_up_1 = np.abs(Z_up_1 - 1)\n",
    "\n",
    "    # X_tilde_up\n",
    "    X_[np.where(W == 0)[0], z_d_dim:] *= Z_up_0\n",
    "    X_[np.where(W == 1)[0], z_d_dim:] *= Z_up_1\n",
    "\n",
    "    X_[X_ == 0] = missing_value\n",
    "\n",
    "    # OUTCOMES\n",
    "    theta_y0 = np.random.randn(d)\n",
    "    theta_y1 = np.random.randn(d)\n",
    "\n",
    "    Y0 = np.sum(np.abs(X * theta_y0), 1)\n",
    "    Y1 = np.sum(np.abs(X * theta_y1), 1)\n",
    "\n",
    "    Y = (\n",
    "        np.array([Y0[i] if w == 0 else Y1[i] for i, w in enumerate(W)])\n",
    "        + np.random.randn(n) * 0.1\n",
    "    )\n",
    "\n",
    "    CATE = Y1 - Y0\n",
    "\n",
    "    return X, X_, Y0, Y1, Y, CATE, W, Z_up_1, Z_up_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor(missing_value):\n",
    "    # return LinearRegression()\n",
    "    # return LassoCV()\n",
    "    # return SVR()\n",
    "    return XGBRegressor(missing=missing_value, eval_metric=\"logloss\")\n",
    "\n",
    "\n",
    "def get_classifier(missing_value):\n",
    "    # return SVC(probability=True)\n",
    "    # return LogisticRegression()\n",
    "    return XGBClassifier(\n",
    "        use_label_encoder=False, missing=missing_value, eval_metric=\"logloss\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_imputer(missing_value, strategy=\"simple\"):\n",
    "    if strategy == \"iterative\":\n",
    "        return IterativeImputer(\n",
    "            max_iter=1500, tol=15e-4, random_state=None, missing_values=missing_value\n",
    "        )\n",
    "\n",
    "    return SimpleImputer(missing_values=0, strategy=\"mean\")\n",
    "\n",
    "\n",
    "learners = {\n",
    "    \"T\": lambda missing_value: metalearners.TLearner(\n",
    "        models=get_regressor(missing_value)\n",
    "    ),\n",
    "    \"X\": lambda missing_value: XLearner(\n",
    "        models=get_regressor(missing_value),\n",
    "        propensity_model=get_classifier(missing_value),\n",
    "        cate_models=get_regressor(missing_value),\n",
    "    ),\n",
    "    \"S\": lambda missing_value: metalearners.SLearner(\n",
    "        overall_model=get_regressor(missing_value),\n",
    "    ),\n",
    "    \"R\": lambda missing_value: NonParamDML(\n",
    "        model_y=get_regressor(missing_value),\n",
    "        model_t=get_classifier(missing_value),\n",
    "        model_final=get_regressor(missing_value),\n",
    "        discrete_treatment=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate(ground_truth, estimate, W):\n",
    "    PEHE = np.sqrt(((estimate - ground_truth) ** 2).mean())\n",
    "    PEHE_0 = np.sqrt(((estimate[W == 0] - ground_truth[W == 0]) ** 2).mean())\n",
    "    PEHE_1 = np.sqrt(((estimate[W == 1] - ground_truth[W == 1]) ** 2).mean())\n",
    "    return PEHE, PEHE_0, PEHE_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP. SETTINGS\n",
    "train_size = 5000\n",
    "runs = 30\n",
    "\n",
    "n = 10000\n",
    "d = 20\n",
    "z_d_dim = 10\n",
    "amount_of_missingness = 0.1\n",
    "treatment_balance = 0.5\n",
    "missing_value = -1\n",
    "\n",
    "learner = \"T\"\n",
    "\n",
    "\n",
    "# DEBUG SETTINGS\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6a79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PEHE_impute_all = []\n",
    "PEHE_impute_nothing = []\n",
    "PEHE_impute_smartly = []\n",
    "PEHE_impute_wrongly = []\n",
    "for _ in tqdm(range(runs)):\n",
    "    X, X_, Y0, Y1, Y, CATE, W, _, _ = generate_data(\n",
    "        n,\n",
    "        d,\n",
    "        z_d_dim,\n",
    "        amount_of_missingness,\n",
    "        treatment_balance,\n",
    "        missing_value=missing_value,\n",
    "    )\n",
    "\n",
    "    assert 10 < train_size < len(X)\n",
    "\n",
    "    X_train, Y_train, W_train, CATE_train = (\n",
    "        X_[:train_size],\n",
    "        Y[:train_size],\n",
    "        W[:train_size],\n",
    "        CATE[:train_size],\n",
    "    )\n",
    "    X_test, Y_test, W_test, CATE_test = (\n",
    "        X_[train_size:],\n",
    "        Y[train_size:],\n",
    "        W[train_size:],\n",
    "        CATE[train_size:],\n",
    "    )\n",
    "\n",
    "    # IMPUTE ALL\n",
    "    imputer = get_imputer(missing_value)\n",
    "    imputer.fit(X_train)\n",
    "    X_train_preprocessed = imputer.transform(X_train)\n",
    "    X_test_preprocessed = imputer.transform(X_test)\n",
    "\n",
    "    est_impute_all = learners[learner](missing_value)\n",
    "    est_impute_all.fit(Y_train, W_train, X=X_train_preprocessed)\n",
    "\n",
    "    te = est_impute_all.effect(X_test_preprocessed)\n",
    "    PEHE_impute_all.append(evaluate(CATE_test, te, W_test))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"all\", X_train.min())\n",
    "\n",
    "    # IMPUTE NOTHING\n",
    "    treatment_effects_impute_nothing = []\n",
    "    X_train_preprocessed = X_train.copy()\n",
    "    X_test_preprocessed = X_test.copy()\n",
    "\n",
    "    est_impute_nothing = learners[learner](missing_value)\n",
    "    est_impute_nothing.fit(Y_train, W_train, X=X_train_preprocessed)\n",
    "\n",
    "    te = est_impute_nothing.effect(X_test_preprocessed)\n",
    "    PEHE_impute_nothing.append(evaluate(CATE_test, te, W_test))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"nothing\", X_train.min())\n",
    "\n",
    "    # IMPUTE SMARTLY\n",
    "    treatment_effects_impute_smartly = []\n",
    "    imputer_smart = get_imputer(missing_value)\n",
    "    imputer_smart.fit(X_train[:, z_d_dim:])\n",
    "\n",
    "    X_train_preprocessed = X_train.copy()\n",
    "    X_test_preprocessed = X_test.copy()\n",
    "\n",
    "    X_train_preprocessed[:, z_d_dim:] = imputer_smart.transform(X_train[:, z_d_dim:])\n",
    "    X_test_preprocessed[:, z_d_dim:] = imputer_smart.transform(X_test[:, z_d_dim:])\n",
    "\n",
    "    est_impute_smartly = learners[learner](missing_value)\n",
    "    est_impute_smartly.fit(Y_train, W_train, X=X_train_preprocessed)\n",
    "\n",
    "    te = est_impute_smartly.effect(X_test_preprocessed)\n",
    "    PEHE_impute_smartly.append(evaluate(CATE_test, te, W_test))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"smart down\", X_train[:, :z_d_dim].min())\n",
    "        print(\"smart up\", X_train[:, z_d_dim:].min())\n",
    "\n",
    "    # IMPUTE WRONGLY\n",
    "    treatment_effects_impute_wrongly = []\n",
    "    imputer_wrongly = get_imputer(missing_value)\n",
    "    imputer_wrongly.fit(X_train[:, :z_d_dim])\n",
    "\n",
    "    X_train_preprocessed = X_train.copy()\n",
    "    X_test_preprocessed = X_test.copy()\n",
    "\n",
    "    X_train_preprocessed[:, :z_d_dim] = imputer_wrongly.transform(X_train[:, :z_d_dim])\n",
    "    X_test_preprocessed[:, :z_d_dim] = imputer_wrongly.transform(X_test[:, :z_d_dim])\n",
    "\n",
    "    est_impute_wrongly = learners[learner](missing_value)\n",
    "    est_impute_wrongly.fit(Y_train, W_train, X=X_train_preprocessed)\n",
    "\n",
    "    te = est_impute_wrongly.effect(X_test_preprocessed)\n",
    "    PEHE_impute_wrongly.append(evaluate(CATE_test, te, W_test))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"wrong down\", X_train[:, :z_d_dim].min())\n",
    "        print(\"wrong up\", X_train[:, z_d_dim:].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029231e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEHE_impute_all = np.array(PEHE_impute_all)\n",
    "PEHE_impute_nothing = np.array(PEHE_impute_nothing)\n",
    "PEHE_impute_smartly = np.array(PEHE_impute_smartly)\n",
    "PEHE_impute_wrongly = np.array(PEHE_impute_wrongly)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\n",
    "        \"#   ALL IMPUTATION  \",\n",
    "        np.mean(PEHE_impute_all[:, i]),\n",
    "        np.std(PEHE_impute_all[:, i]),\n",
    "    )\n",
    "    print(\n",
    "        \"#   NO IMPUTATION   \",\n",
    "        np.mean(PEHE_impute_nothing[:, i]),\n",
    "        np.std(PEHE_impute_nothing[:, i]),\n",
    "    )\n",
    "    print(\n",
    "        \"#   SMART IMPUTATION\",\n",
    "        np.mean(PEHE_impute_smartly[:, i]),\n",
    "        np.std(PEHE_impute_smartly[:, i]),\n",
    "    )\n",
    "    print(\n",
    "        \"#   WRONG IMPUTATION\",\n",
    "        np.mean(PEHE_impute_wrongly[:, i]),\n",
    "        np.std(PEHE_impute_wrongly[:, i]),\n",
    "    )\n",
    "    print(f\"#   ---------------- PEHE_{i}\")  # ignore last printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS (temp, see below)\n",
    "\n",
    "# T-Learner (30 runs | n=5000:5000 | XGBoost)\n",
    "#                    PEHE                 std\n",
    "#   ALL IMPUTATION   0.45278096486338737  0.1870098285385787\n",
    "#   NO IMPUTATION    0.3907391370195506   0.14171094562076403\n",
    "#   SMART IMPUTATION 0.3712755863794166   0.13224640582290206\n",
    "#   WRONG IMPUTATION 0.3934260413765053   0.13899847163776846\n",
    "#   ---------------- PEHE_0\n",
    "#   ALL IMPUTATION   0.5140391774257107   0.25787108248825386\n",
    "#   NO IMPUTATION    0.42550403610099863  0.18329028240118667\n",
    "#   SMART IMPUTATION 0.40195085962895044  0.1706762110629843\n",
    "#   WRONG IMPUTATION 0.4308186072053363   0.18588081720873728\n",
    "#   ---------------- PEHE_1\n",
    "#   ALL IMPUTATION   0.36312889849827507  0.12865710551108703\n",
    "#   NO IMPUTATION    0.3429487353355224   0.11383623011386533\n",
    "#   SMART IMPUTATION 0.32806068833598945  0.10911623962765807\n",
    "#   WRONG IMPUTATION 0.34132695814288355  0.1056961605089806\n",
    "\n",
    "\n",
    "# X-Learner (10 runs | n=5000:5000 | XGBoost)\n",
    "#                    PEHE                 std\n",
    "#   ALL IMPUTATION   0.2266373835636965   0.05506510645041428\n",
    "#   NO IMPUTATION    0.22517104405764674  0.06137348194738814\n",
    "#   SMART IMPUTATION 0.2140350458606493   0.047633315894963994\n",
    "#   WRONG IMPUTATION 0.22049010141019537  0.05121918492129533\n",
    "#   ---------------- PEHE_0\n",
    "#   ALL IMPUTATION   0.2547728226031637   0.07351525604295465\n",
    "#   NO IMPUTATION    0.24664801136551998  0.07104835200774624\n",
    "#   SMART IMPUTATION 0.23393031514949708  0.05935025115884689\n",
    "#   WRONG IMPUTATION 0.2250017817222049   0.04600639105862219\n",
    "#   ---------------- PEHE_1\n",
    "#   ALL IMPUTATION   0.19324599739406753  0.035763337618871426\n",
    "#   NO IMPUTATION    0.19958311970077378  0.05700321830432496\n",
    "#   SMART IMPUTATION 0.1908529213220182   0.03819124718983082\n",
    "#   WRONG IMPUTATION 0.21491864757558649  0.05898220132085252\n",
    "\n",
    "\n",
    "# S-Learner (10 runs | n=5000:5000 | XGBoost)\n",
    "#                    PEHE                 std\n",
    "#   ALL IMPUTATION   0.2641162154929491   0.11288001339236028\n",
    "#   NO IMPUTATION    0.2793748577304963   0.11555372203809124\n",
    "#   SMART IMPUTATION 0.24833807460990198  0.12194151808293517\n",
    "#   WRONG IMPUTATION 0.27935659139470287  0.11426520897085507\n",
    "#   -----------------\n",
    "#   ALL IMPUTATION   0.2861844708655413   0.13755625212984332\n",
    "#   NO IMPUTATION    0.2896694279534494   0.13411197726048069\n",
    "#   SMART IMPUTATION 0.27540835342994074  0.1562677549217369\n",
    "#   WRONG IMPUTATION 0.2921668215014991   0.1422390730587516\n",
    "#   -----------------\n",
    "#   ALL IMPUTATION   0.23831874782808704  0.0864665280728755\n",
    "#   NO IMPUTATION    0.267208085932947    0.09782183979627333\n",
    "#   SMART IMPUTATION 0.2152925363455575   0.0815447247922131\n",
    "#   WRONG IMPUTATION 0.2632941604831127   0.08599820621825298\n",
    "\n",
    "\n",
    "# TODO\n",
    "# - DONE split PEHE to W=1/0\n",
    "# - rerun with 100 runs to make sure, we want 110%\n",
    "#     reprodocible\n",
    "# - ATE; likely the effect from bias will be greater here\n",
    "# (- combine T/X-Learner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
